{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454c04da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated EXTREMELY challenging advanced datasets with messy features and multiple split files; files are in 'advanced_data/'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zzppo\\AppData\\Local\\Temp\\ipykernel_20852\\436884651.py:159: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '$477.19' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  transactions.at[idx, 'Amount'] = f\"${original_amount:.2f}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import re # For regex in cleaning phone numbers\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "num_customers = 1000\n",
    "num_stores = 20\n",
    "start_date = pd.to_datetime('2025-01-01')\n",
    "end_date = pd.to_datetime('2025-12-31')\n",
    "\n",
    "# Sample names and emojis for non-ASCII characters\n",
    "sample_names = ['Alice', 'Bob', 'Charlie', 'Dana', 'Eve', 'Frank', 'Grace', 'Heidi']\n",
    "emojis = ['', ' ðŸ˜Š', ' Â©', ' Ã¶', ' â„¢', ' Â®'] # More emojis/symbols\n",
    "\n",
    "# Extended Phone number formats\n",
    "phone_formats = [\n",
    "    np.nan, '555-1234', '5551234', '555.1234', '(555) 123-4567',\n",
    "    '+1 555 987 6543', '010-12345678', '999-ERROR', 'SHORT', 'LONG-NUMBER-TOO-LONG'\n",
    "]\n",
    "\n",
    "# Extended City names with inconsistencies\n",
    "city_names = ['Cairo', 'Alexandria', 'Giza']\n",
    "city_variations = {\n",
    "    'Cairo': ['Cairo', 'Cario', 'Cai', 'Al-Qahira'],\n",
    "    'Alexandria': ['Alexandria', 'Alex.', 'Alexandira', 'Al-Iskandariyah'],\n",
    "    'Giza': ['Giza', 'Gizah', 'Jizah', 'Gizeh']\n",
    "}\n",
    "\n",
    "# 1. Generate customers table with more challenging issues\n",
    "customers_data = []\n",
    "# Generate base customers\n",
    "for i in range(1, num_customers + 1):\n",
    "    name = random.choice(sample_names) + random.choice(emojis)\n",
    "    customers_data.append({\n",
    "        'custID': i,\n",
    "        'Customer Name': name,\n",
    "        'PreferredStore': np.random.randint(1, num_stores + 1),\n",
    "        'join_date': pd.to_datetime(random.choice(pd.date_range(start_date, end_date))),\n",
    "        'Phone #': random.choice(phone_formats)\n",
    "    })\n",
    "\n",
    "customers = pd.DataFrame(customers_data)\n",
    "\n",
    "# Introduce missing values in 'Customer Name'\n",
    "customers.loc[customers.sample(frac=0.05).index, 'Customer Name'] = np.nan\n",
    "\n",
    "# Introduce some fuzzy duplicates (same customer, slight name/phone variations)\n",
    "num_fuzzy_duplicates = int(num_customers * 0.03) # 3% fuzzy duplicates\n",
    "fuzzy_indices = customers.sample(n=num_fuzzy_duplicates, random_state=10).index\n",
    "for idx in fuzzy_indices:\n",
    "    original_row = customers.loc[idx].copy()\n",
    "    new_cust_id = customers['custID'].max() + 1\n",
    "    new_row = original_row.copy()\n",
    "    new_row['custID'] = new_cust_id\n",
    "\n",
    "    # Introduce variations\n",
    "    if random.random() < 0.5: # 50% chance of name variation\n",
    "        new_row['Customer Name'] = original_row['Customer Name'].replace('e', 'a', 1).replace('o', 'u', 1) if isinstance(original_row['Customer Name'], str) else 'Jane Doe'\n",
    "    if random.random() < 0.5: # 50% chance of phone variation\n",
    "        if isinstance(original_row['Phone #'], str) and '-' in original_row['Phone #']:\n",
    "            new_row['Phone #'] = original_row['Phone #'].replace('-', '')\n",
    "        elif isinstance(original_row['Phone #'], str) and ' ' in original_row['Phone #']:\n",
    "            new_row['Phone #'] = original_row['Phone #'].replace(' ', '.')\n",
    "        else:\n",
    "            new_row['Phone #'] = random.choice(phone_formats) # new random format\n",
    "\n",
    "    customers = pd.concat([customers, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "\n",
    "# 2. Generate stores table with more challenging issues\n",
    "stores_data = []\n",
    "for i in range(1, num_stores + 1):\n",
    "    original_city = random.choice(city_names)\n",
    "    city_entry = random.choice(city_variations[original_city]) + random.choice(['', ' ']) # Add extra space sometimes\n",
    "    stores_data.append({\n",
    "        'STORE_id': i,\n",
    "        'StoreName': f\"Store_{i}{random.choice(['', 'â„¢', 'Â§', ' Limited', ' Corp.'])}\", # More variations\n",
    "        'Size': np.random.choice(['Small', 'Medium', 'Large']),\n",
    "        'City ': city_entry, # Use varied city entries\n",
    "        'Opening': pd.to_datetime(random.choice(pd.date_range(start_date, end_date)))\n",
    "    })\n",
    "stores = pd.DataFrame(stores_data)\n",
    "\n",
    "# Introduce inconsistent formats in the 'Size' column (some uppercase, some lowercase)\n",
    "stores.loc[stores.sample(frac=0.10, random_state=1).index, 'Size'] = \\\n",
    "    stores.loc[stores.sample(frac=0.10, random_state=1).index, 'Size'].apply(lambda x: x.upper())\n",
    "stores.loc[stores.sample(frac=0.05, random_state=2).index, 'Size'] = \\\n",
    "    stores.loc[stores.sample(frac=0.05, random_state=2).index, 'Size'].apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "# 3. Generate transactions table with hierarchical and time-series data\n",
    "num_transactions = 10000\n",
    "payment_methods = ['Cash', 'Credit Card', 'Mobile Payment', 'Card', 'CC', 'Mobile', 'Unknown', np.nan] # More variations\n",
    "transactions = pd.DataFrame({\n",
    "    'TxnID': np.arange(1, num_transactions + 1),\n",
    "    'custID': np.random.choice(customers['custID'], num_transactions),\n",
    "    'STORE_id': np.random.choice(stores['STORE_id'], num_transactions),\n",
    "    'TxnDate': pd.to_datetime(\n",
    "        np.random.choice(\n",
    "            pd.date_range(start_date, end_date, freq='D'),\n",
    "            num_transactions\n",
    "        )\n",
    "    ),\n",
    "    'Amount': np.round(np.random.uniform(5, 500, num_transactions), 2),\n",
    "    'PaymentMethod': np.random.choice(payment_methods, num_transactions)\n",
    "})\n",
    "\n",
    "# Introduce missing values in 'PaymentMethod'\n",
    "transactions.loc[transactions.sample(frac=0.05, random_state=2).index, 'PaymentMethod'] = np.nan\n",
    "\n",
    "# Add inconsistent 'PaymentMethod' format (lowercase + trailing space)\n",
    "transactions.loc[np.random.choice(transactions.index, size=100, replace=False), 'PaymentMethod'] = 'creditcard ' # trailing space\n",
    "transactions.loc[np.random.choice(transactions.index, size=50, replace=False), 'PaymentMethod'] = 'cash ' # new trailing space\n",
    "\n",
    "\n",
    "# 4. Introduce orphan transactions (invalid foreign keys) - Remains challenging\n",
    "orphan_indices = transactions.sample(frac=0.02, random_state=3).index\n",
    "transactions.loc[\n",
    "    orphan_indices, 'custID'\n",
    "] = customers['custID'].max() + np.random.randint(1, 50, size=len(orphan_indices)) # Use max + 1 to ensure truly new IDs\n",
    "transactions.loc[\n",
    "    orphan_indices, 'STORE_id'\n",
    "] = stores['STORE_id'].max() + np.random.randint(1, 10, size=len(orphan_indices)) # Use max + 1\n",
    "\n",
    "\n",
    "# 5. Introduce conflicting dates (TxnDate before join_date AND TxnDate before store Opening)\n",
    "conflict_indices_cust = transactions.sample(frac=0.02, random_state=4).index\n",
    "for idx in conflict_indices_cust:\n",
    "    cust_id = transactions.at[idx, 'custID']\n",
    "    join_date_series = customers.loc[customers['custID'] == cust_id, 'join_date']\n",
    "    if not join_date_series.empty:\n",
    "        transactions.at[idx, 'TxnDate'] = join_date_series.iloc[0] - pd.Timedelta(days=random.randint(15, 60))\n",
    "    else:\n",
    "        transactions.at[idx, 'TxnDate'] = start_date - pd.Timedelta(days=random.randint(30, 90))\n",
    "\n",
    "# Conflict: Transaction before store opening date\n",
    "conflict_indices_store = transactions.sample(frac=0.01, random_state=5).index # 1% additional conflicts\n",
    "for idx in conflict_indices_store:\n",
    "    store_id = transactions.at[idx, 'STORE_id']\n",
    "    opening_date_series = stores.loc[stores['STORE_id'] == store_id, 'Opening']\n",
    "    if not opening_date_series.empty:\n",
    "        transactions.at[idx, 'TxnDate'] = opening_date_series.iloc[0] - pd.Timedelta(days=random.randint(5, 30))\n",
    "    else:\n",
    "        # If the transaction's STORE_id is orphaned, set TxnDate to a date earlier than start_date\n",
    "        transactions.at[idx, 'TxnDate'] = start_date - pd.Timedelta(days=random.randint(40, 100))\n",
    "\n",
    "\n",
    "# 6. Add messy 'Amount' formats for some transactions (mix numbers, strings with currency, other text)\n",
    "amount_indices = transactions.sample(frac=0.07, random_state=6).index # Increased percentage\n",
    "for idx in amount_indices:\n",
    "    original_amount = transactions.at[idx, 'Amount']\n",
    "    choice = random.random()\n",
    "    if choice < 0.4:\n",
    "        transactions.at[idx, 'Amount'] = f\"${original_amount:.2f}\"\n",
    "    elif choice < 0.6:\n",
    "        transactions.at[idx, 'Amount'] = f\"â‚¬{original_amount:.2f}\" # Euro\n",
    "    elif choice < 0.7:\n",
    "        transactions.at[idx, 'Amount'] = f\"Â£{original_amount:.2f}\" # Pound\n",
    "    elif choice < 0.8:\n",
    "        transactions.at[idx, 'Amount'] = \"FREE\" # Textual amount\n",
    "    elif choice < 0.9:\n",
    "        transactions.at[idx, 'Amount'] = \"ZERO\"\n",
    "    else:\n",
    "        transactions.at[idx, 'Amount'] = \"N/A\" # Another text type\n",
    "\n",
    "\n",
    "# Create the base 'advanced_data' folder\n",
    "base_output_folder = 'advanced_data_v2'\n",
    "os.makedirs(base_output_folder, exist_ok=True)\n",
    "\n",
    "# 7. Split transactions by month and save each file under 'advanced_data/monthly_transactions_v2'\n",
    "monthly_transactions_folder = os.path.join(base_output_folder, 'monthly_transactions_v2')\n",
    "os.makedirs(monthly_transactions_folder, exist_ok=True)\n",
    "for month, group in transactions.groupby(transactions['TxnDate'].dt.to_period('M')):\n",
    "    filename = f\"{monthly_transactions_folder}/transactions_{month}.csv\"\n",
    "    group.to_csv(filename, index=False, encoding='utf-8')\n",
    "\n",
    "# 8. Split customers by PreferredStore and save each file under 'advanced_data/customers_by_store_v2'\n",
    "customers_by_store_folder = os.path.join(base_output_folder, 'customers_by_store_v2')\n",
    "os.makedirs(customers_by_store_folder, exist_ok=True)\n",
    "for store_id, group in customers.groupby('PreferredStore'):\n",
    "    filename = f\"{customers_by_store_folder}/customers_store_{store_id}.csv\"\n",
    "    group.to_csv(filename, index=False, encoding='utf-8')\n",
    "\n",
    "# 9. Split stores by City and save each file under 'advanced_data/stores_by_city_v2'\n",
    "# Clean the 'City ' column name for grouping (temporary for grouping, analyst needs to handle this in real cleaning)\n",
    "# Note: The raw city names are still messy in the saved files\n",
    "stores_for_city_split = stores.copy()\n",
    "stores_for_city_split['City_Cleaned'] = stores_for_city_split['City '].str.strip().apply(\n",
    "    lambda x: next((k for k, v in city_variations.items() if x in v), x)\n",
    ")\n",
    "stores_by_city_folder = os.path.join(base_output_folder, 'stores_by_city_v2')\n",
    "os.makedirs(stores_by_city_folder, exist_ok=True)\n",
    "for city_cleaned, group in stores_for_city_split.groupby('City_Cleaned'):\n",
    "    sanitized_city = re.sub(r'[^a-zA-Z0-9_]', '', city_cleaned.replace(\" \", \"_\")) # More robust sanitization\n",
    "    filename = f\"{stores_by_city_folder}/stores_{sanitized_city}.csv\"\n",
    "    # Save the original 'City ' column, not the temporary cleaned one\n",
    "    group.drop(columns=['City_Cleaned']).to_csv(filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"âœ… Generated EXTREMELY challenging advanced datasets with messy features and multiple split files; files are in 'advanced_data/'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
